---
title: "Advanced Clustering Methods from Post Data"
author: "Bradley Katcher and Aneesh Sandhir"
output: html_document
---

Millions of sites exist that allow individuals to post and answer questions. From personal blogs to sites like Reddit and Stack Exachange, there are a plethora of opportunities for people to interact and help eachother throughout the internet. In doing so, enclave-like internet communities begin to form, where the same individuals respond to the same types of questions and virtually interact with the same kinds of individuals. Throughout this tutorial, we will employ data mining techniques to demonstrate how to identify a set of individuals that engage with the same kind of content online. 

As a case study to demonstrate the formation of online communities, we will walk through an analysis of [CrossValidated](https://stats.stackexchange.com/) users. The data set that we have is pulled from Stack Exchange's Data Explorer Tool. It contains all posts answered by users with a reputation score over 10,000. The data consist of 42,739 individual observations (at the question level), answered by 104 different users. 

```{r eval = TRUE, include = FALSE}
library(ggplot2)
data <- read.csv(url('https://raw.githubusercontent.com/bkatcher4/DataMiningFinal/master/question_metrics_power_users.csv'))

#remove accepted_a_id
data <- subset(data, select = -c(accepted_a_id) )
```


We can preview the first six rows of data here: 
```{r eval = TRUE, echo = FALSE}
head(data)
names <- c(23:126)
data[,names] <- lapply(data[,names] , factor)
```

We first conducted some feature engineering. As you can see, we have the question number, an indicator for whether the question was answered, the number of characters and words in the body of the question and the title of the question, the number of tags on the post, the number of tags in the body, the number of tags in the title, the number of paragraphs in the body, the number of code snippets, the number of plots, the number of "stop words" (words that don't mean anything in a natural langauage processing context such as articles, prepositions, etc.), the number of question marks in the title, the number of question marks in the body, the minimum tag popularity, the minimum tag popularity score, the minimum tag answered ratio, maximum tag popularity, maximum tag popularity score, maximum tag answered ratio, and one-hot encoding for whether a given user answered the question. 

This data represents characteristics about the questions and is very similar to data that could be scraped and created (via feature engieering) from any given forum site, such as Reddit, Quora, or another quesiton and answer-oriented site. From this data, we seek to get information regarding our users. In order to do this, we are going to run a series of regression models predicting their likelihood of answering a question. We do this in order to obtain beta coefficients for users which indicate their propensity for answering the question. Since the outcome is binary regarding whether someone answered the question, we intend to utilize logistic regression. These coefficients represent the log odds ratio of the impact of a certain variable (for example, the verbosity of the post), and so we know that users with similar coefficient values have similar propensities to respond to a certain question with that characteristic. 

Therefore, we build a dataset containing 104 observations (one for each user) contianing their vector of betas. We run a seperate logistic regression for each user where the outcome is whether they answer the question and the predictors are a vector of question characteristics. While in theory, we could employ other types of regression, i.e. logsitic LASSO, logistic ridge, or logistic elastic net (thereby adding a penalty term to the regression, and beneficial for conducting variable selection), we do not because of certain users only answering a few questions, e.g. User 8 only answers 4 questions, and to run 10-fold crossvalidation on that would be impractical due to some folds thus not having any "answered" outcomes, and the length of time it would take to run 104 10-fold crossvalidation regressions.

# Network Analysis:

### Spectral Clustering
```{r eval = TRUE}
questions <- read.csv(url('https://raw.githubusercontent.com/bkatcher4/DataMiningFinal/master/question_metrics_power_users.csv'))

adjacency_matrix <- matrix(0,(ncol(questions)-23),(ncol(questions)-23))
for(user_a in 1:104)
{
  for(user_b in 1:104)
  {
    if(user_a > user_b)
    {
      answered_by_user_a <- questions[,c(user_a+23)]
      answered_by_user_a <- ((answered_by_user_a > 0)*1)
      answered_by_user_b <- questions[,c(user_b+23)]
      answered_by_user_b <- ((answered_by_user_b > 0)*1)
      adjacency_matrix[user_a, user_b] <- sum(questions[,c(user_a+23)] == 1 & questions[,c(user_b+23)] == 1)
      adjacency_matrix[user_b, user_a] <- sum(questions[,c(user_b+23)] == 1 & questions[,c(user_a+23)] == 1)
    }
  }
}

degree_matrix <- matrix(0,(ncol(questions)-23),(ncol(questions)-23))
for(user_a in 1:104)
{
  degree_matrix[user_a, user_a] <- sum((adjacency_matrix[user_a,] > 0)*1)
}

laplacian_matrix <- degree_matrix - adjacency_matrix
eigen_object <- eigen(laplacian_matrix)
number_of_participants <-  nrow(laplacian_matrix)

max_number_of_clusters <- 52
SSE = numeric(max_number_of_clusters)
for(number_of_clusters in 1:max_number_of_clusters){
  X = eigen_object$vectors[, number_of_participants-(1:number_of_clusters)] # get the smallest k non-zero eigevectors
  km = kmeans(X, centers=number_of_clusters, nstart = 100) # run kmeans
  SSE[number_of_clusters] = km$tot.withinss   # calculate the SSE
}

plot(1:max_number_of_clusters, SSE, type='o', las=1, xlab="k")
plot(eigen_object$values[number_of_participants-(1:max_number_of_clusters)], xlab="k", ylab="eigenvalues")
plot(diff(eigen_object$values[number_of_participants-(1:max_number_of_clusters)]))

number_of_clusters <- 8
X = eigen_object$vectors[, number_of_participants-(1:number_of_clusters)] # get the smallest k non-zero eigevectors
km = kmeans(X, centers=number_of_clusters, nstart=100)  # run k-means 

single_user_clusers <- which(km$size == 1)
outlier_indicies <- rep(0, length(single_user_clusers))
outlier_user_ids <- rep(0, length(single_user_clusers))

#for(index in 1:length(single_user_clusers))
#{
  #outlier_indicies[index] <- which(km$cluster == single_user_clusers[index])  
  #outlier_user_ids[index] <- labels[which(km$cluster == single_user_clusers[index])]  
#}
```

# Regression Model:

### Simple Logistic Regression
```{r eval = TRUE}
library(tibble)

# intialize tibble to store coefficients
linear_model_betas <- tibble(name.repair=c("X1", 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11'))

# loop across all users
for(user in 1:104){
  
  (user)
  #subset data frame
  answered_by_user <- data[,c(3:22, (user+22))]
  
  # run model
  model <- glm(answered_by_user[,ncol(answered_by_user)] ~ n_paragraphs_body + n_code_snippets + n_stop_words_body + verbosity + n_tags_title + ques_marks_title + min_tag_answered_ratio + max_tag_popularity + max_tag_answered_ratio + max_tag_popularity_score, family = binomial(link = "logit"), data = answered_by_user)
  
  # store beta coefficients in a local
  beta_i <- model$coefficients
  
  # add beta coefficients to data frame
  linear_model_betas <- cbind(linear_model_betas, beta_i)
}

#remove X labels
linear_model_betas <- linear_model_betas[,-1]

#rename variables in tibble 
tibble_names <- names(data[,names])
colnames(linear_model_betas) <- tibble_names
```

Using this regression, we end up with a dataset containing data regarding the impact of each of the factors we identify on the likelihood that a given user will answer a question. We can preview the dataset below:

```{r eval = TRUE}
# Transpose the dataset so that the users are the observations and the varaibles are the variables:
t_lmb = t(linear_model_betas)
head(t_lmb)
linear_model_betas = t(linear_model_betas)
```


Now that we have our dataset, we can begin conducint clustering analysis to see similarities in the characterisitcs of users who answer similar questions. In order to do this, we decide to explore different metrics of distance between users, as well as different ways of clustering users. We will begin by employing hierarchical clustering, then K-means cluster, and explore other types of clustering such as c-means clustering and spectral clustering. 

We first begin by exploring distance between various users based on the coefficients. Given the number of users, it is im impractical to put all of the users on one graph, therefore, we will divide the users into relative thirds. We compute a euclidian distance between the users, and the more red the block is, the further the individuals are from eachother, whereas the more blue, the closer the individuals are to eachother

#Computing Distance between All Users:  
```{r eval = TRUE}
library('factoextra')
library(ggplot2)

manhattan_distance <- rep(0, ((nrow(laplacian_matrix)^2/2)-(nrow(laplacian_matrix)/2)))
euclidean_distance <- rep(0, ((nrow(laplacian_matrix)^2/2)-(nrow(laplacian_matrix)/2)))
cosine_distance <- rep(0, ((nrow(laplacian_matrix)^2/2)-(nrow(laplacian_matrix)/2)))
index <- 0  
for(user_a in 1:104)
{
  for(user_b in 1:104)
  {
    if(user_a > user_b)
    {
      index <- index + 1
      manhattan_distance[index] <- sum(abs(linear_model_betas[user_a,] - linear_model_betas[user_b,]))
      euclidean_distance[index] <- sqrt(sum((linear_model_betas[user_a,] - linear_model_betas[user_b,])^2))
      cosine_distance[index] <- 1 - (sum(linear_model_betas[user_a,] * linear_model_betas[user_b,]) / (sum(linear_model_betas[user_a,]^2)*sum(linear_model_betas[user_b,]^2)))
    }
  }
}

res.dist <- get_dist(t_lmb[1:35,], stand = TRUE, method = "euclidian")
fviz_dist(res.dist, 
   gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))

res.dist2 <- get_dist(t_lmb[36:70,], stand = TRUE, method = "euclidian")
fviz_dist(res.dist2, 
   gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))


res.dist3 <- get_dist(t_lmb[71:104,], stand = TRUE, method = "euclidian")
fviz_dist(res.dist3, 
   gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))

```



## Hierarchical Clustering 

We begin by computing the pairwise dissimilarity between users, which is defined as the Euclidian distance between points. From this, we conduct hierarchical clustering and create a dendrogram from agglomerative hierarchical clustering. In doing so, we use complete linkage, which computes cluster dissimilarity as the largest dissimilarity between pairs in two sets. Therefore, on the dendrogram below, the height corresponds to the dissimlarity between the most dissimlar points in the two clusters. 

In order to choose the number of clusters (K), we look for regions in the dendrogram where gaps/changes occur in the height of merges, as a large jump in height corresponds to a high dissimilarity between the solutions for k and k-1 clusters. As can be seen from the plot that compares the height to the number of clusters, we see a jump between K=6 and K=7, so we elect to choose K=7 for our clustering. 

```{r eval = TRUE}
#Calculate Maximum distance
distance = dist(t_lmb, method="euclidian")

#clustering
clust = hclust(distance, method="complete")      # complete linkage

#plots
plot(as.dendrogram(clust), las=1, leaflab="none", main = "Complete Linkage Clustering Dendrogram", ylab = "Height")
n = length(clust$height)     # get number of merges

plot(n:1, clust$height, type='o', xlab="K", ylab="height", las=1, 
     xlim=c(1, 52))
points(7, clust$height[n-6], col="red", pch=19) # K=7

clusterNum <- cutree(clust, k=7)     # cut so K=7
clusterData <- data.frame(cbind(clusterNum,t_lmb))
```

From here, we can determine the number of individuals in each cluster, telling us how these most active users group together:

```{r eval = TRUE}
# Produces a tbale of the number of individuals in each cluster:
table(clusterNum)
```

We can also plot who these users are based on the two highest principal components:

```{r eval = TRUE}
pca_hier = data.frame(cbind(prcomp(t_lmb, scale=FALSE)$x[, 1:2], clusterNum))  # first 2 PC
ggplot(pca_hier, aes(PC1, PC2)) + geom_point(aes(color = factor(clusterNum)))

```

In the graph above, we see that XXXX

From this result, we can learn a lot about which CrossValidated users try to answer similar questions. There is a group of exactly half of the top users that tend to answer the same kinds of questions, competing with eachother for the best answer and reputation points, while there are several smaller groups as well. Groups of 24, 10, and 15 people. In addition, there are three individuals that seem to be "in a league of their own", meaning that they do not seem to answer the same questions as others. It's worthwhile to try and explore who these individuals are:

```{r eval = TRUE}
rownames(clusterData[clusterData$clusterNum>4,])
```
We can see that these three "unique" users are users number 8, 251, and 8373. 

## K-Means Clustering
We now move into another type of clustering, K-means. To begin, we scale the vector of coefficients. Next, we run kmeans for multiple K and plot the value of K against the SSE in order to look for an "elbow", which is the place wehre we choose the opitmal K. 

```{r eval = TRUE}
library(stats)

# define the data
km_lmb = t_lmb                

#-- Run kmeans for multiple K
Kmax = 40                                 
SSE = numeric(Kmax)                       # initiate SSE vector
for(k in 1:Kmax){
  km = kmeans(km_lmb, centers=k, nstart=25)    # use 25 starts
  SSE[k] = km$tot.withinss                # get SSE
}

#-- Plot results
plot(1:Kmax, SSE, type='o', las=1, xlab="K")
```

Based on the graph above, it looks like the optimal K is k=7, as that's where there is a kink point in the graph. 

```{r eval = TRUE}
set.seed(42069)
km = kmeans(km_lmb, centers=7, nstart=25) # use K=7
km_lmb = data.frame(cbind(km$cluster, t_lmb))
table(km$cluster)
```

Based on these results, we see the same general pattern as above. While we don't see one large group, we see four groups of rouhgly 20-25 people, a smaller group of 7 people, and 2 outliers of only one person:

```{r eval = TRUE}
rownames(km_lmb[km_lmb$V1==1 | km_lmb$V1==6,])
```

We discover that user 8 and user 251 are once again outliers, which is the same result that we observe with hierarchical clustering. We can visualize the clusters again based on a plot of the two principal components, with coloring by cluster:

```{r eval = TRUE}
pca_km = data.frame(cbind(prcomp(t_lmb, scale=FALSE)$x[, 1:2], km$cluster))  # first 2 PC
pca_km$cluster <- as.factor(pca_km$V3)
ggplot(pca_km, aes(PC1, PC2)) + geom_point(aes(color = cluster))
```

## Fuzzy C-Means

```{r eval = TRUE}
set.seed(75)

library("cluster")
library("factoextra")
library("magrittr")
library('ppclust')


# define the data
fcm_lmb = t_lmb                

#-- Run c-means for K = 7 (using the same number of clusters as for k-means and hierarchical)
fcm = fcm(x = fcm_lmb, centers = 7, nstart = 10)   # use 10 starts


# create cluster plot 
res.fcm2 <- ppclust2(fcm, "kmeans")
fviz_cluster(res.fcm2, data = fcm_lmb, 
  ellipse.type = "convex",
  palette = "jco",
  labelsize = 0,
  repel = FALSE)

```

With c-means, we obtain the probability of picking each observation being in each cluster, which can be viewed below for the first few observations:
```{r eval = TRUE}
head(fcm$u)
```

We can also see the distance between each point and the centroid of the clusters for the first few observations:
```{r eval = TRUE}
head(fcm$d)
```

We can see that using fuzzy c-means and picking the probabilistically most likely cluster gives us a more even distribution, many groups are around 15-25 people, with an outlier of one person.
```{r eval = TRUE}
fcm$csize
```

We can see the outlier is, not clustered with anyone else, and again it is User 8:
```{r eval = TRUE}
fcm_res = data.frame(cbind(t_lmb, fcm$cluster))
rownames(fcm_res[fcm_res$V12==7,])
```

# Conclusions and Takeaways:


